use-grammars = { only = [
  "awk", "bash", "regex",
  "cmake", "cpp", "c", "make",
  "diff", "gitattributes", "git-commit", "git-config", "gitignore", "git-rebase",
  "dockerfile",
  "hosts", "passwd",
  "json", "toml", "xml", "yaml",
  "markdown", "mermaid",
  "css", "html", "javascript",
  "python",
  "rust",
  "typst"
]}

[[language]]
name = "dockerfile"
auto-format = true
language-servers = ["efm"]

[[language]]
name = "rust"
language-servers = ["rust-analyzer", "lsp-ai"]
auto-format = true

[[language]]
name = "markdown"
language-servers = ["harper-ls", "marksman", "lsp-ai"]
# comment-token = "-"
comment-tokens = ["-", "+", "*", "1.", ">", "- [ ]"]
auto-format = true
formatter = { command = "prettier", args = ["--parser", "markdown", "--tab-width", "4"] }

[[language]]
name = "python"
language-servers = ["pylsp", "lsp-ai"]
auto-format = true
formatter = { command = "bash", args = ["-c", "isort - | black -"] }

[[language]]
name = "c"
auto-format = true
formatter = { command = "clang-format", args = ["--style=Chromium"] }
file-types = ["c", "p4"]

[[language]]
name = "cpp"
auto-format = true
formatter = { command = "clang-format", args = ["--style=Chromium"] }

[[language]]
name = "typst"
scope = "source.typst"
injection-regex = "^typ(st)?$"
roots = ["typst.toml"]
comment-token = "//"
file-types = ["typ"]
indent = { tab-width = 2, unit = " " }
language-servers = ["typst-lsp"]
text-width = 100
rulers = [100]
soft-wrap.wrap-at-text-width = true

[language.auto-pairs]
'(' = ')'
'{' = '}'
'[' = ']'
'"' = '"'
'`' = '`'
'$' = '$'

[[grammar]]
name = "typst"
source = { path = "/home/lagoyd/src/uben0-tree-sitter-typst" }

[language-server.ltex-lsp]
command = "ltex-ls"

[language-server.typst-lsp]
command = "typst-lsp"

[language-server.pylsp]
[language-server.pylsp.config]
pylsp.plugins.pycodestyle.enabled = false
# pylsp.plugins.pycodestyle.maxLineLength = 88
pylsp.plugins.black.enabled = true
pylsp.plugins.flake8.enabled = true

[language-server.efm]
command = "efm-langserver"
args = ["-c", "/root/.config/helix/efm.yaml"]

[language-server.harper-ls]
command = "harper-ls"
args = ["--stdio"]

[language-server.lsp-ai]
command = "lsp-ai"
timeout = 300
[language-server.lsp-ai.config]
memory.file_store = {}
# memory.vector_store = {}
# models.model1.type = "ollama"
# models.model1.chat_endpoint = "http://ollama:11434/api/chat"
# models.model1.generate_endpoint = "http://ollama:11434/api/generate"
# # models.model1.model = "starcoder2:15b"
# models.model1.model = "deepseek-coder-v2"
# models.model1.model = "hf.co/second-state/StarCoder2-15B-GGUF:Q8_0"
# models.model1.model = "hf.co/second-state/StarCoder2-3B-GGUF:Q8_0"
# completion.model = "model1"
# completion.parameters.max_context = 4096
# completion.parameters.options.num_predict = 64
# completion.parameters.options.stop = ["\n"]
[language-server.lsp-ai.config.models.devstral]
type = "ollama"
chat_endpoint = "http://ollama:11434/api/chat"
generate_endpoint = "http://ollama:11434/api/generate"
model = "devstral"
[language-server.lsp-ai.config.models.dc2]
type = "ollama"
chat_endpoint = "http://ollama:11434/api/chat"
generate_endpoint = "http://ollama:11434/api/generate"
model = "deepseek-coder-v2"
[language-server.lsp-ai.config.models.sc2]
type = "ollama"
chat_endpoint = "http://ollama:11434/api/chat"
generate_endpoint = "http://ollama:11434/api/generate"
model = "starcoder2:15b"
[language-server.lsp-ai.config.models.qc25]
type = "ollama"
chat_endpoint = "http://ollama:11434/api/chat"
generate_endpoint = "http://ollama:11434/api/generate"
model = "qwen2.5-coder:32b"
[language-server.lsp-ai.config.models.dr14]
type = "ollama"
chat_endpoint = "http://ollama:11434/api/chat"
generate_endpoint = "http://ollama:11434/api/generate"
model = "deepseek-r1:14b"
[language-server.lsp-ai.config.models.dr32]
type = "ollama"
chat_endpoint = "http://ollama:11434/api/chat"
generate_endpoint = "http://ollama:11434/api/generate"
model = "deepseek-r1:32b"
[language-server.lsp-ai.config.completion]
model = "devstral"
parameters.max_context = 4096
parameters.options.num_predict = 512
parameters.options.stop = ["\n"]
[[language-server.lsp-ai.config.actions]]
action_display_name = "Complete devstral"
model = "devstral"
parameters.max_context = 4096
parameters.options.num_predict = 512
parameters.options.stop = ["<file_sep>", "\n\n"]
[[language-server.lsp-ai.config.actions]]
action_display_name = "Complete DC2"
model = "dc2"
parameters.max_context = 4096
parameters.options.num_predict = 512
parameters.options.stop = ["<file_sep>", "\n\n"]
[[language-server.lsp-ai.config.actions]]
action_display_name = "Complete QC2.5"
model = "qc25"
parameters.max_context = 4096
parameters.options.num_predict = 512
parameters.options.stop = ["<file_sep>", "\n\n"]
[[language-server.lsp-ai.config.actions]]
action_display_name = "FIM Complete QC2.5"
model = "qc25"
parameters.max_context = 4096
parameters.fim.start = "<|fim_prefix|>"
parameters.fim.middle = "<|fim_middle|>"
parameters.fim.end = "<|fim_suffix|>"
parameters.options.num_predict = 512
parameters.options.stop = ["<file_sep>", "\n\n"]
[[language-server.lsp-ai.config.actions]]
action_display_name = "FIM Complete DC2"
model = "dc2"
parameters.max_context = 4096
parameters.fim.start = "<｜fim▁begin｜>"
parameters.fim.middle = "<｜fim▁hole｜>"
parameters.fim.end = "<｜fim▁end｜>"
parameters.options.num_predict = 512
parameters.options.stop = ["<file_sep>", "\n\n"]
[[language-server.lsp-ai.config.actions]]
action_display_name = "FIM Complete devstral"
model = "devstral"
parameters.max_context = 4096
parameters.fim.start = "<｜fim▁begin｜>"
parameters.fim.middle = "<｜fim▁hole｜>"
parameters.fim.end = "<｜fim▁end｜>"
parameters.options.num_predict = 512
parameters.options.stop = ["<file_sep>", "\n\n"]
[[language-server.lsp-ai.config.actions]]
action_display_name = "Complete DS-r1 32b"
model = "dr32"
parameters.max_context = 4096
# parameters.options.num_predict = 512
parameters.messages = [
    { role = "System", content = "You are an AI coding assistant. Your task is to complete code snippets. The user's cursor position is marked by \"<CURSOR>\". Follow these steps:\n\n1. Analyze the code context and the cursor position.\n2. Provide your chain of thought reasoning, wrapped in <reasoning> tags. Include thoughts about the cursor position, what needs to be completed, and any necessary formatting.\n3. Determine the appropriate code to complete the current thought, including finishing partial words or lines.\n4. Replace \"<CURSOR>\" with the necessary code, ensuring proper formatting and line breaks.\n5. Wrap your code solution in <answer> tags.\n\nYour response should always include both the reasoning and the answer. Pay special attention to completing partial words or lines before adding new lines of code.\n\n<examples>\n<example>\nUser input:\n--main.py--\n# A function that reads in user inpu<CURSOR>\n\nResponse:\n<reasoning>\n1. The cursor is positioned after \"inpu\" in a comment describing a function that reads user input.\n2. We need to complete the word \"input\" in the comment first.\n3. After completing the comment, we should add a new line before defining the function.\n4. The function should use Python's built-in `input()` function to read user input.\n5. We'll name the function descriptively and include a return statement.\n</reasoning>\n\n<answer>t\ndef read_user_input():\n    user_input = input(\"Enter your input: \")\n    return user_input\n</answer>\n</example>\n\n<example>\nUser input:\n--main.py--\ndef fibonacci(n):\n    if n <= 1:\n        return n\n    else:\n        re<CURSOR>\n\n\nResponse:\n<reasoning>\n1. The cursor is positioned after \"re\" in the 'else' clause of a recursive Fibonacci function.\n2. We need to complete the return statement for the recursive case.\n3. The \"re\" already present likely stands for \"return\", so we'll continue from there.\n4. The Fibonacci sequence is the sum of the two preceding numbers.\n5. We should return the sum of fibonacci(n-1) and fibonacci(n-2).\n</reasoning>\n\n<answer>turn fibonacci(n-1) + fibonacci(n-2)</answer>\n</example>\n</examples>"},
    { role = "user", content = "{CODE}" }
]
post_process.extractor = "(?s)<answer>\n*(.*?)</answer>"
[[language-server.lsp-ai.config.actions]]
action_display_name = "Complete DS-r1 14b"
model = "dr14"
parameters.max_context = 4096
# parameters.options.num_predict = 512
parameters.messages = [
    { role = "System", content = "You are an AI coding assistant. Your task is to complete code snippets. The user's cursor position is marked by \"<CURSOR>\". Follow these steps:\n\n1. Analyze the code context and the cursor position.\n2. Provide your chain of thought reasoning, wrapped in <reasoning> tags. Include thoughts about the cursor position, what needs to be completed, and any necessary formatting.\n3. Determine the appropriate code to complete the current thought, including finishing partial words or lines.\n4. Replace \"<CURSOR>\" with the necessary code, ensuring proper formatting and line breaks.\n5. Wrap your code solution in <answer> tags.\n\nYour response should always include both the reasoning and the answer. Pay special attention to completing partial words or lines before adding new lines of code.\n\n<examples>\n<example>\nUser input:\n--main.py--\n# A function that reads in user inpu<CURSOR>\n\nResponse:\n<reasoning>\n1. The cursor is positioned after \"inpu\" in a comment describing a function that reads user input.\n2. We need to complete the word \"input\" in the comment first.\n3. After completing the comment, we should add a new line before defining the function.\n4. The function should use Python's built-in `input()` function to read user input.\n5. We'll name the function descriptively and include a return statement.\n</reasoning>\n\n<answer>t\ndef read_user_input():\n    user_input = input(\"Enter your input: \")\n    return user_input\n</answer>\n</example>\n\n<example>\nUser input:\n--main.py--\ndef fibonacci(n):\n    if n <= 1:\n        return n\n    else:\n        re<CURSOR>\n\n\nResponse:\n<reasoning>\n1. The cursor is positioned after \"re\" in the 'else' clause of a recursive Fibonacci function.\n2. We need to complete the return statement for the recursive case.\n3. The \"re\" already present likely stands for \"return\", so we'll continue from there.\n4. The Fibonacci sequence is the sum of the two preceding numbers.\n5. We should return the sum of fibonacci(n-1) and fibonacci(n-2).\n</reasoning>\n\n<answer>turn fibonacci(n-1) + fibonacci(n-2)</answer>\n</example>\n</examples>"},
    { role = "user", content = "{CODE}" }
]
post_process.extractor = "(?s)<answer>\n*(.*?)</answer>"
[[language-server.lsp-ai.config.chat]]
trigger = "!C"
action_display_name = "Chat"
model = "dr32"
parameters.max_context = 4096
parameters.max_tokens = 1024
parameters.messages = [
  { "role" = "system", "content" = "You are a code assistant chatbot. The user will ask you for assistance coding and you will do you best to answer succinctly and accurately" }
]
